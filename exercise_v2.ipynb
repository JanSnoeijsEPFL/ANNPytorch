{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.9982  0.9982  0.0000  0.0000\n",
      " 0.9996  0.9996  0.9982  0.9982\n",
      " 1.9992  1.9992  0.9996  0.9996\n",
      " 3.0002  2.0020  1.9992  1.9992\n",
      " 4.0026  2.0048  2.0020  2.0020\n",
      "[torch.FloatTensor of size 5x4]\n",
      " \n",
      " 1  1  0  0\n",
      " 0  0  1  1\n",
      " 1  1  0  0\n",
      " 1  0  1  1\n",
      " 1  0  0  0\n",
      "[torch.FloatTensor of size 5x4]\n",
      " \n",
      " 1  1  0  0\n",
      " 1  1  1  1\n",
      " 2  2  1  1\n",
      " 3  2  2  2\n",
      " 4  2  2  2\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n",
      "\n",
      " 0.9982  0.0000  0.9982  0.9982\n",
      " 1.9978  0.9982  1.9978  1.9978\n",
      " 2.9988  0.9996  2.9988  2.9988\n",
      " 3.0030  1.9992  4.0012  3.0030\n",
      " 3.0072  3.0002  5.0050  4.0054\n",
      "[torch.FloatTensor of size 5x4]\n",
      " \n",
      " 1  0  1  1\n",
      " 1  1  1  1\n",
      " 1  0  1  1\n",
      " 0  1  1  0\n",
      " 0  1  1  1\n",
      "[torch.FloatTensor of size 5x4]\n",
      " \n",
      " 1  0  1  1\n",
      " 2  1  2  2\n",
      " 3  1  3  3\n",
      " 3  2  4  3\n",
      " 3  3  5  4\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#reLU function definition\n",
    "def reLU(x):\n",
    "    output = torch.max(x, torch.Tensor(x.size(0), x.size(1)).fill_(0))\n",
    "    return output\n",
    "\n",
    "#derivative of reLU\n",
    "def deriv_reLU(x):\n",
    "    p = torch.max(x, torch.Tensor(x.size(0), x.size(1)).fill_(0))\n",
    "    q = torch.ceil(p)\n",
    "    output = torch.min(q, torch.Tensor(x.size(0), x.size(1)).fill_(1))\n",
    "    return output\n",
    "\n",
    "# Training data & Test data initialization\n",
    "SeqL=5\n",
    "NbSeq=4\n",
    "torch.manual_seed(4)\n",
    "X_train = torch.round(torch.rand(SeqL, NbSeq))\n",
    "X_test = torch.round(torch.rand(SeqL, NbSeq))\n",
    "T_train = torch.Tensor(SeqL, NbSeq)\n",
    "T_test = torch.Tensor(SeqL, NbSeq)\n",
    "for k in range(SeqL):\n",
    "    T_train[k,:] = torch.t(torch.sum(X_train.narrow(0,0,k+1),0).view(-1,1))\n",
    "    T_test[k,:] = torch.t(torch.sum(X_test.narrow(0,0,k+1),0).view(-1,1))\n",
    "#parameters initialization\n",
    "l_r=0.000001\n",
    "Wxs = torch.Tensor(2*np.random.random(1))\n",
    "Wrec = torch.Tensor(2*np.random.random(1)) \n",
    "\n",
    "#training\n",
    "\n",
    "layer_1=torch.Tensor(SeqL,NbSeq).fill_(0)\n",
    "layer_0 = X_train\n",
    "s = torch.Tensor(SeqL, NbSeq).fill_(0)\n",
    "l1_pp_s = torch.Tensor(SeqL, NbSeq).fill_(0)\n",
    "l1_pp_x = torch.Tensor(SeqL, NbSeq).fill_(0)\n",
    "l1_sum, l1_sum2 = torch.Tensor(4), torch.Tensor(4)\n",
    "\n",
    "#forward-pass equations\n",
    "for iter in range(1000):\n",
    "    # first number of sequence - no recurrent term\n",
    "    s[0,:]=Wxs*layer_0.narrow(0,0,1)\n",
    "    layer_1[0,:]=reLU(s[0,:].view(-1,1))\n",
    "    for k in range(1, SeqL):\n",
    "        s[k,:]=Wxs*layer_0.narrow(0,k,1)+Wrec*layer_1.narrow(0,k-1,1)\n",
    "        layer_1[k,:]=reLU(s[k,:].view(-1,1))\n",
    "      \n",
    "    #considered error : 1/2 (T-layer_1)^2\n",
    "    l1_err_deriv = T_train - layer_1 \n",
    "    \n",
    "    for n in range(SeqL):\n",
    "    \n",
    "        #product of partial derivatives and multiplication of each product by s(k-1):\n",
    "        #for the first line its 0 because there is no s(k-1) term\n",
    "        for k in range(1, n):\n",
    "            #additional Wrec for the unconsidered last line \n",
    "            l1_pp_s[k,:] = deriv_reLU(s.narrow(0,k,1))*layer_1.narrow(0,k-1,1)* \\\n",
    "            (torch.cumprod(Wrec*deriv_reLU(s.narrow(0,k,n-k)),0)[n-k-1,:])\n",
    "\n",
    "        #sum over k = 1 to k=n+1\n",
    "        l1_sum += torch.sum(l1_pp_s,0)\n",
    "\n",
    "        #same for x\n",
    "        #this time the first line can be computed because s(k) depends on x(k) and not on x(k-1)\n",
    "        for k in range(0, n):\n",
    "            l1_pp_x[k,:] = deriv_reLU(s.narrow(0,k,1))*layer_0.narrow(0,k,1)* \\\n",
    "            (torch.cumprod(Wrec*deriv_reLU(s.narrow(0,k,n-k)),0)[n-k-1,:])\n",
    "\n",
    "        #sum over k = 1 to k = n+1\n",
    "        l1_sum2 += torch.sum(l1_pp_x,0)\n",
    "    \n",
    "    #new coefficients\n",
    "    Wrec += l_r*torch.sum(l1_err_deriv*torch.t((l1_sum/SeqL).view(-1,1)))\n",
    "    Wxs += l_r*torch.sum(l1_err_deriv*torch.t((l1_sum2/SeqL).view(-1,1)))\n",
    "\n",
    "    #end of training\n",
    "#test phase\n",
    "print(layer_1, X_train, T_train)\n",
    "layer_0 = X_test\n",
    "s[0,:]=Wxs*layer_0.narrow(0,0,1)\n",
    "layer_1[0,:]=reLU(s[0,:].view(-1,1))\n",
    "for k in range(1,SeqL):\n",
    "    s[k,:]=Wxs*layer_0.narrow(0,k,1)+Wrec*layer_1.narrow(0,k-1,1)\n",
    "    layer_1[k,:]=reLU(s[k,:].view(-1,1))\n",
    "      \n",
    "    #end of test phase\n",
    "print(layer_1, X_test, T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
